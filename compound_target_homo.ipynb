{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chakr\\Desktop\\BTP\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import networkx as nx\n",
    "from torch.nn import BatchNorm1d\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from transformers import T5Tokenizer, T5Model\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 -> Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('new_chembl_inhibit_drug_target_1.csv')  # Replace with your CSV path\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 -> Construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chakr\\AppData\\Local\\Temp\\ipykernel_1068\\4134255447.py:82: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  data['target'].x = torch.tensor(target_features, dtype=torch.float).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Set up the device (use CUDA if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# Initialize HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Add compound nodes\n",
    "compounds = df['compound_chembl_id'].unique()\n",
    "compound_map = {compound: i for i, compound in enumerate(compounds)}\n",
    "\n",
    "# Add target nodes\n",
    "targets = df['target_uniprot_id'].unique()\n",
    "target_map = {target: i for i, target in enumerate(targets)}\n",
    "\n",
    "# Add edges (compound <-> target) ensuring undirected edges\n",
    "compound_indices = np.array(df['compound_chembl_id'].map(compound_map).values)\n",
    "target_indices = np.array(df['target_uniprot_id'].map(target_map).values)\n",
    "\n",
    "# Convert to PyTorch tensors and move to GPU\n",
    "edge_index = torch.tensor(np.vstack([compound_indices, target_indices]), dtype=torch.long).to(device)\n",
    "edge_index_rev = torch.tensor(np.vstack([target_indices, compound_indices]), dtype=torch.long).to(device)\n",
    "\n",
    "# Assign undirected edges to the graph\n",
    "data['target', 'interacts', 'compound'].edge_index = edge_index_rev\n",
    "data['compound', 'interacts', 'target'].edge_index = edge_index\n",
    "\n",
    "# Load PPI data\n",
    "cancer_ppi_data = pd.read_csv('cancer_ppi_combined.csv')\n",
    "\n",
    "# Add Protein Nodes\n",
    "existing_proteins = set(target_map.keys())\n",
    "\n",
    "# Filter only valid PPI interactions\n",
    "cancer_ppi_data_filtered = cancer_ppi_data[\n",
    "    (cancer_ppi_data['node1_uniprot_id'].isin(existing_proteins)) | \n",
    "    (cancer_ppi_data['node2_uniprot_id'].isin(existing_proteins))\n",
    "]\n",
    "\n",
    "# Map PPI edges to indices\n",
    "ppi_edges = []\n",
    "len_target_map = len(target_map)\n",
    "\n",
    "for _, row in cancer_ppi_data_filtered.iterrows():\n",
    "    node1, node2 = row['node1_uniprot_id'], row['node2_uniprot_id']\n",
    "    \n",
    "    # Assign indices if not already mapped\n",
    "    if node1 not in target_map:\n",
    "        target_map[node1] = len_target_map\n",
    "        len_target_map += 1\n",
    "    if node2 not in target_map:\n",
    "        target_map[node2] = len_target_map\n",
    "        len_target_map += 1\n",
    "\n",
    "    # Append undirected edges\n",
    "    ppi_edges.append([target_map[node1], target_map[node2]])\n",
    "    ppi_edges.append([target_map[node2], target_map[node1]])  # Add reverse edge\n",
    "\n",
    "# Convert PPI edges to tensor\n",
    "ppi_edge_index = torch.tensor(ppi_edges, dtype=torch.long).T.to(device)\n",
    "\n",
    "# Assign undirected PPI edges to the graph\n",
    "data['target', 'interacts', 'target'].edge_index = ppi_edge_index\n",
    "\n",
    "data['compound'].x = torch.load('compound_features.pt')\n",
    "\n",
    "import pickle\n",
    "\n",
    "embeddings_dict_merged = pickle.load(open(\"embeddings_dict.pkl\", \"rb\"))\n",
    "\n",
    "# Add protein features\n",
    "target_features = []\n",
    "\n",
    "# Load target features in order of the target_map value (index)\n",
    "for idx in range(len(target_map)):\n",
    "    protein = [protein for protein, i in target_map.items() if i == idx][0]\n",
    "    if protein in embeddings_dict_merged:\n",
    "        target_features.append(embeddings_dict_merged[protein])\n",
    "    else:\n",
    "        target_features.append(np.zeros(1024))\n",
    "\n",
    "data['target'].x = torch.tensor(target_features, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct targets from the target_map\n",
    "targets = [protein for protein, _ in sorted(target_map.items(), key=lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2123\n"
     ]
    }
   ],
   "source": [
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2123\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings_dict_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      node1_uniprot_id node2_uniprot_id\n",
      "113             Q6GPI1           P09683\n",
      "114             Q6GPI1           P01298\n",
      "201             P60709           Q99835\n",
      "221             P60709           Q9NYK1\n",
      "222             P60709           Q9NZQ7\n",
      "...                ...              ...\n",
      "39749           Q99759           P00533\n",
      "39761           P46109           P00533\n",
      "39788           Q9Y2R2           P00533\n",
      "39799           Q15047           Q9Y6K1\n",
      "39801           Q9Y6K1           Q15047\n",
      "\n",
      "[3570 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(cancer_ppi_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70902\n"
     ]
    }
   ],
   "source": [
    "print(len(df['compound_chembl_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2123\n",
      "46660\n"
     ]
    }
   ],
   "source": [
    "print(len(target_map))\n",
    "print(len(compound_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46660\n"
     ]
    }
   ],
   "source": [
    "print(len(compounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CHEMBL340': 0, 'CHEMBL4302': 1, 'CHEMBL2046258': 2, 'CHEMBL2835': 3, 'CHEMBL2363062': 4, 'CHEMBL3632452': 5, 'CHEMBL4523999': 6, 'CHEMBL4296661': 7, 'CHEMBL4005': 8, 'CHEMBL2842': 9, 'CHEMBL1163125': 10, 'CHEMBL2599': 11, 'CHEMBL4523988': 12, 'CHEMBL1937': 13, 'CHEMBL1829': 14, 'CHEMBL325': 15, 'CHEMBL333': 16, 'CHEMBL321': 17, 'CHEMBL2973': 18, 'CHEMBL3231': 19, 'CHEMBL4835': 20, 'CHEMBL4940': 21, 'CHEMBL2364162': 22, 'CHEMBL1947': 23, 'CHEMBL1163101': 24, 'CHEMBL5145': 25, 'CHEMBL2111432': 26, 'CHEMBL3130': 27, 'CHEMBL2292': 28, 'CHEMBL1865': 29, 'CHEMBL5432': 30, 'CHEMBL3399911': 31, 'CHEMBL267': 32, 'CHEMBL2002': 33, 'CHEMBL3024': 34, 'CHEMBL4523993': 35, 'CHEMBL6006': 36, 'CHEMBL3085620': 37, 'CHEMBL2362979': 38, 'CHEMBL1795184': 39, 'CHEMBL1973': 40, 'CHEMBL5608': 41, 'CHEMBL4898': 42, 'CHEMBL2815': 43, 'CHEMBL4072': 44, 'CHEMBL3105': 45, 'CHEMBL2111389': 46, 'CHEMBL3116': 47, 'CHEMBL4527': 48, 'CHEMBL1907611': 49, 'CHEMBL1871': 50, 'CHEMBL6166': 51, 'CHEMBL4895': 52, 'CHEMBL3594': 53, 'CHEMBL1795117': 54, 'CHEMBL2148': 55, 'CHEMBL2971': 56, 'CHEMBL5393': 57, 'CHEMBL4662': 58, 'CHEMBL3124734': 59, 'CHEMBL2157850': 60, 'CHEMBL2147': 61, 'CHEMBL1825': 62, 'CHEMBL260': 63, 'CHEMBL4361': 64, 'CHEMBL2611': 65, 'CHEMBL3524': 66, 'CHEMBL4506': 67, 'CHEMBL1951': 68, 'CHEMBL3267': 69, 'CHEMBL4079': 70, 'CHEMBL5023': 71, 'CHEMBL1255126': 72, 'CHEMBL4523996': 73, 'CHEMBL4630': 74, 'CHEMBL3880': 75, 'CHEMBL2111367': 76, 'CHEMBL3691': 77, 'CHEMBL3991501': 78, 'CHEMBL3883316': 79, 'CHEMBL1907605': 80, 'CHEMBL3430904': 81, 'CHEMBL3430888': 82, 'CHEMBL2046267': 83, 'CHEMBL279': 84, 'CHEMBL3961': 85, 'CHEMBL2185': 86, 'CHEMBL2068': 87, 'CHEMBL3864': 88, 'CHEMBL4026': 89, 'CHEMBL4303': 90, 'CHEMBL3553': 91, 'CHEMBL3623': 92, 'CHEMBL1919': 93, 'CHEMBL3038499': 94, 'CHEMBL4267': 95, 'CHEMBL1906': 96, 'CHEMBL4523458': 97, 'CHEMBL3717': 98, 'CHEMBL335': 99, 'CHEMBL1978': 100, 'CHEMBL4247': 101, 'CHEMBL2111387': 102, 'CHEMBL1981': 103, 'CHEMBL203': 104, 'CHEMBL3973': 105, 'CHEMBL1974': 106, 'CHEMBL4979': 107, 'CHEMBL5251': 108, 'CHEMBL2095165': 109, 'CHEMBL1163108': 110, 'CHEMBL5903': 111, 'CHEMBL5543': 112, 'CHEMBL4225': 113, 'CHEMBL5407': 114, 'CHEMBL5084': 115, 'CHEMBL2828': 116, 'CHEMBL4937': 117, 'CHEMBL5422': 118, 'CHEMBL5957': 119, 'CHEMBL2321621': 120, 'CHEMBL2146296': 121, 'CHEMBL2146301': 122, 'CHEMBL262': 123, 'CHEMBL3351': 124, 'CHEMBL4355': 125, 'CHEMBL3038474': 126, 'CHEMBL1907602': 127, 'CHEMBL3038469': 128, 'CHEMBL3038471': 129, 'CHEMBL3038472': 130, 'CHEMBL301': 131, 'CHEMBL3055': 132, 'CHEMBL4630753': 133, 'CHEMBL3883323': 134, 'CHEMBL3038473': 135, 'CHEMBL4106169': 136, 'CHEMBL3351215': 137, 'CHEMBL3559692': 138, 'CHEMBL5697': 139, 'CHEMBL2041': 140, 'CHEMBL2695': 141, 'CHEMBL2150844': 142, 'CHEMBL4600': 143, 'CHEMBL4439': 144, 'CHEMBL228': 145, 'CHEMBL2959': 146, 'CHEMBL2250': 147, 'CHEMBL5014': 148, 'CHEMBL2111363': 149, 'CHEMBL2563': 150, 'CHEMBL2716': 151, 'CHEMBL4296076': 152, 'CHEMBL3580522': 153, 'CHEMBL4598': 154, 'CHEMBL4576': 155, 'CHEMBL3629': 156, 'CHEMBL1862': 157, 'CHEMBL5699': 158, 'CHEMBL1795116': 159, 'CHEMBL1913': 160, 'CHEMBL2095217': 161, 'CHEMBL4295801': 162, 'CHEMBL4295761': 163, 'CHEMBL4296266': 164, 'CHEMBL6144': 165, 'CHEMBL1806': 166, 'CHEMBL2189121': 167, 'CHEMBL2276': 168, 'CHEMBL4852': 169, 'CHEMBL3886': 170, 'CHEMBL2439': 171, 'CHEMBL2007625': 172, 'CHEMBL2345': 173, 'CHEMBL3038498': 174, 'CHEMBL3137262': 175, 'CHEMBL1936': 176, 'CHEMBL1844': 177, 'CHEMBL4105838': 178, 'CHEMBL1781': 179, 'CHEMBL1935': 180, 'CHEMBL2093865': 181, 'CHEMBL2803': 182, 'CHEMBL2331054': 183, 'CHEMBL4523344': 184, 'CHEMBL5971': 185, 'CHEMBL3831201': 186, 'CHEMBL3797': 187, 'CHEMBL5747': 188, 'CHEMBL4630818': 189, 'CHEMBL3885525': 190, 'CHEMBL5939': 191, 'CHEMBL6136': 192, 'CHEMBL2094258': 193, 'CHEMBL2487': 194, 'CHEMBL2769': 195, 'CHEMBL5255': 196, 'CHEMBL5331': 197, 'CHEMBL3038510': 198, 'CHEMBL2189117': 199, 'CHEMBL4282': 200, 'CHEMBL2163183': 201, 'CHEMBL4888448': 202, 'CHEMBL2094135': 203, 'CHEMBL2189110': 204, 'CHEMBL2096618': 205, 'CHEMBL3834': 206, 'CHEMBL3650': 207, 'CHEMBL4142': 208, 'CHEMBL2742': 209, 'CHEMBL1868': 210, 'CHEMBL1957': 211, 'CHEMBL258': 212, 'CHEMBL3905': 213, 'CHEMBL2007': 214, 'CHEMBL2689': 215, 'CHEMBL5568': 216, 'CHEMBL5314': 217, 'CHEMBL4618': 218, 'CHEMBL2052031': 219, 'CHEMBL4050': 220, 'CHEMBL1902': 221, 'CHEMBL3706': 222, 'CHEMBL3313835': 223, 'CHEMBL4523603': 224, 'CHEMBL4523295': 225, 'CHEMBL3038470': 226, 'CHEMBL4261': 227, 'CHEMBL3142': 228, 'CHEMBL4106138': 229, 'CHEMBL3145': 230, 'CHEMBL1075097': 231, 'CHEMBL5921': 232, 'CHEMBL230': 233, 'CHEMBL3784': 234, 'CHEMBL4523512': 235, 'CHEMBL6154': 236, 'CHEMBL3883300': 237, 'CHEMBL3883320': 238, 'CHEMBL4508': 239, 'CHEMBL5918': 240, 'CHEMBL3721': 241, 'CHEMBL2850': 242, 'CHEMBL5776': 243, 'CHEMBL3108647': 244, 'CHEMBL2508': 245, 'CHEMBL2608': 246, 'CHEMBL4415': 247, 'CHEMBL3308911': 248, 'CHEMBL4708': 249, 'CHEMBL3627592': 250, 'CHEMBL2343': 251, 'CHEMBL2428': 252, 'CHEMBL5600': 253, 'CHEMBL5408': 254, 'CHEMBL1795186': 255, 'CHEMBL4879459': 256, 'CHEMBL4739695': 257, 'CHEMBL4106140': 258, 'CHEMBL1293257': 259, 'CHEMBL5987': 260, 'CHEMBL4739852': 261, 'CHEMBL2406899': 262, 'CHEMBL4105786': 263, 'CHEMBL1075145': 264, 'CHEMBL4722': 265, 'CHEMBL5627': 266, 'CHEMBL284': 267, 'CHEMBL5866': 268, 'CHEMBL5719': 269, 'CHEMBL2079846': 270, 'CHEMBL2111429': 271, 'CHEMBL1824': 272, 'CHEMBL3774295': 273, 'CHEMBL3836': 274, 'CHEMBL3885599': 275, 'CHEMBL4523428': 276, 'CHEMBL3788': 277, 'CHEMBL4482': 278, 'CHEMBL5416': 279, 'CHEMBL1907601': 280, 'CHEMBL2111455': 281, 'CHEMBL3018': 282, 'CHEMBL1892': 283, 'CHEMBL4481': 284, 'CHEMBL3218': 285, 'CHEMBL2111353': 286, 'CHEMBL2111288': 287, 'CHEMBL1907600': 288, 'CHEMBL3108645': 289, 'CHEMBL4531': 290, 'CHEMBL1293299': 291, 'CHEMBL3779760': 292, 'CHEMBL2095188': 293, 'CHEMBL2722': 294, 'CHEMBL5580': 295, 'CHEMBL1075189': 296, 'CHEMBL5749': 297, 'CHEMBL6174': 298, 'CHEMBL1908383': 299, 'CHEMBL2243': 300, 'CHEMBL3259467': 301, 'CHEMBL5989': 302, 'CHEMBL4660': 303, 'CHEMBL3334416': 304, 'CHEMBL4523435': 305, 'CHEMBL4295828': 306, 'CHEMBL4237': 307, 'CHEMBL1075317': 308, 'CHEMBL1075094': 309, 'CHEMBL5464': 310, 'CHEMBL1828': 311, 'CHEMBL3337327': 312, 'CHEMBL2360': 313, 'CHEMBL3108638': 314, 'CHEMBL2431': 315, 'CHEMBL4816': 316, 'CHEMBL2189116': 317, 'CHEMBL4128': 318, 'CHEMBL5122': 319, 'CHEMBL2111416': 320, 'CHEMBL2073663': 321, 'CHEMBL5553': 322, 'CHEMBL2069156': 323, 'CHEMBL6094': 324, 'CHEMBL308': 325, 'CHEMBL4036': 326, 'CHEMBL2094138': 327, 'CHEMBL3579': 328, 'CHEMBL4462': 329, 'CHEMBL2527': 330, 'CHEMBL2708': 331, 'CHEMBL5291961': 332, 'CHEMBL4014': 333, 'CHEMBL2284': 334, 'CHEMBL5024': 335, 'CHEMBL2311243': 336, 'CHEMBL4338': 337, 'CHEMBL2730': 338, 'CHEMBL3363': 339, 'CHEMBL5805': 340, 'CHEMBL4860': 341, 'CHEMBL1293224': 342, 'CHEMBL3832943': 343, 'CHEMBL5291523': 344, 'CHEMBL1908389': 345, 'CHEMBL1163124': 346, 'CHEMBL2997': 347, 'CHEMBL6152': 348, 'CHEMBL1955': 349, 'CHEMBL3166': 350, 'CHEMBL6044': 351, 'CHEMBL2176858': 352, 'CHEMBL2052028': 353, 'CHEMBL2094255': 354, 'CHEMBL3106': 355, 'CHEMBL4575': 356, 'CHEMBL6032': 357, 'CHEMBL3108655': 358, 'CHEMBL6030': 359, 'CHEMBL3833502': 360, 'CHEMBL2083': 361, 'CHEMBL3344': 362, 'CHEMBL3674': 363, 'CHEMBL5685': 364, 'CHEMBL3334418': 365, 'CHEMBL4680030': 366, 'CHEMBL331': 367, 'CHEMBL4680038': 368, 'CHEMBL244': 369, 'CHEMBL3286': 370, 'CHEMBL4105737': 371, 'CHEMBL4296001': 372, 'CHEMBL4295717': 373, 'CHEMBL2146308': 374, 'CHEMBL5285': 375, 'CHEMBL1163123': 376, 'CHEMBL3572': 377, 'CHEMBL2095182': 378, 'CHEMBL2288': 379, 'CHEMBL1907598': 380, 'CHEMBL2096675': 381, 'CHEMBL1293289': 382, 'CHEMBL2424': 383, 'CHEMBL6164': 384, 'CHEMBL4903': 385, 'CHEMBL2976': 386, 'CHEMBL3236': 387, 'CHEMBL3009': 388, 'CHEMBL5659': 389, 'CHEMBL1075163': 390, 'CHEMBL4045': 391, 'CHEMBL2116': 392, 'CHEMBL1909484': 393, 'CHEMBL4802012': 394, 'CHEMBL2093861': 395, 'CHEMBL3885556': 396, 'CHEMBL2793': 397, 'CHEMBL6000': 398, 'CHEMBL1075102': 399, 'CHEMBL5554': 400, 'CHEMBL5403': 401, 'CHEMBL1255131': 402, 'CHEMBL4523964': 403, 'CHEMBL3775': 404, 'CHEMBL3217390': 405, 'CHEMBL5284': 406, 'CHEMBL3920': 407, 'CHEMBL2996': 408, 'CHEMBL299': 409, 'CHEMBL3830': 410, 'CHEMBL3430907': 411, 'CHEMBL5880': 412, 'CHEMBL3832644': 413, 'CHEMBL2095174': 414, 'CHEMBL4179': 415, 'CHEMBL3784906': 416, 'CHEMBL1795151': 417, 'CHEMBL3663': 418, 'CHEMBL2157': 419, 'CHEMBL2331053': 420, 'CHEMBL3861': 421, 'CHEMBL4141': 422, 'CHEMBL5784': 423, 'CHEMBL3137261': 424, 'CHEMBL3885541': 425, 'CHEMBL1770034': 426, 'CHEMBL4311': 427, 'CHEMBL4524': 428, 'CHEMBL5330': 429, 'CHEMBL5579': 430, 'CHEMBL5701': 431, 'CHEMBL5261': 432, 'CHEMBL1075195': 433, 'CHEMBL5914': 434, 'CHEMBL5970': 435, 'CHEMBL3137283': 436, 'CHEMBL3885523': 437, 'CHEMBL1907': 438, 'CHEMBL4488': 439, 'CHEMBL3817721': 440, 'CHEMBL3530': 441, 'CHEMBL3438': 442, 'CHEMBL2916': 443, 'CHEMBL1741218': 444, 'CHEMBL5169188': 445, 'CHEMBL4718': 446, 'CHEMBL1993': 447, 'CHEMBL4901': 448, 'CHEMBL2413': 449, 'CHEMBL5855': 450, 'CHEMBL4522': 451, 'CHEMBL2777': 452, 'CHEMBL3045': 453, 'CHEMBL4677': 454, 'CHEMBL4625': 455, 'CHEMBL2535': 456, 'CHEMBL3529': 457, 'CHEMBL1741220': 458, 'CHEMBL3233': 459, 'CHEMBL4394': 460, 'CHEMBL4703': 461, 'CHEMBL3425389': 462, 'CHEMBL4105731': 463, 'CHEMBL2094128': 464, 'CHEMBL4106127': 465, 'CHEMBL4523': 466, 'CHEMBL5406': 467, 'CHEMBL2782': 468, 'CHEMBL5737': 469, 'CHEMBL5461': 470, 'CHEMBL5291977': 471, 'CHEMBL4070': 472, 'CHEMBL5401': 473, 'CHEMBL3885604': 474, 'CHEMBL2023': 475, 'CHEMBL3832942': 476, 'CHEMBL3307223': 477, 'CHEMBL5888': 478, 'CHEMBL4680023': 479, 'CHEMBL5524': 480, 'CHEMBL4523442': 481, 'CHEMBL4599': 482, 'CHEMBL5469': 483, 'CHEMBL3891': 484, 'CHEMBL6031': 485, 'CHEMBL5081': 486, 'CHEMBL3085': 487, 'CHEMBL202': 488, 'CHEMBL2096907': 489, 'CHEMBL4680024': 490, 'CHEMBL2434': 491, 'CHEMBL1169596': 492, 'CHEMBL4147': 493, 'CHEMBL4121': 494, 'CHEMBL2801': 495, 'CHEMBL3829': 496, 'CHEMBL2634': 497, 'CHEMBL3290': 498, 'CHEMBL3385': 499, 'CHEMBL4040': 500, 'CHEMBL3982': 501, 'CHEMBL1841': 502, 'CHEMBL2073': 503, 'CHEMBL3476': 504, 'CHEMBL1991': 505, 'CHEMBL5247': 506, 'CHEMBL3357': 507, 'CHEMBL1938215': 508, 'CHEMBL6167': 509, 'CHEMBL5606': 510, 'CHEMBL3587': 511, 'CHEMBL2964': 512, 'CHEMBL2109': 513, 'CHEMBL4948': 514, 'CHEMBL2171': 515, 'CHEMBL3956': 516, 'CHEMBL2208': 517, 'CHEMBL4670': 518, 'CHEMBL5940': 519, 'CHEMBL3831': 520, 'CHEMBL3125': 521, 'CHEMBL5552': 522, 'CHEMBL1075155': 523, 'CHEMBL4851': 524, 'CHEMBL4309': 525, 'CHEMBL2417353': 526, 'CHEMBL5257': 527, 'CHEMBL2939': 528, 'CHEMBL4674': 529, 'CHEMBL4501': 530, 'CHEMBL2534': 531, 'CHEMBL3667': 532, 'CHEMBL5808': 533, 'CHEMBL5878': 534, 'CHEMBL2595': 535, 'CHEMBL5785': 536, 'CHEMBL1908382': 537, 'CHEMBL2553': 538, 'CHEMBL3906': 539, 'CHEMBL1908384': 540, 'CHEMBL4375': 541, 'CHEMBL5668': 542, 'CHEMBL1795198': 543, 'CHEMBL4105756': 544, 'CHEMBL5388': 545, 'CHEMBL5639': 546, 'CHEMBL5518': 547, 'CHEMBL2417354': 548, 'CHEMBL5332': 549, 'CHEMBL5907': 550, 'CHEMBL1163127': 551, 'CHEMBL1163128': 552, 'CHEMBL2417355': 553, 'CHEMBL4578': 554, 'CHEMBL3627584': 555, 'CHEMBL4487': 556, 'CHEMBL4597': 557, 'CHEMBL5790': 558, 'CHEMBL5667': 559, 'CHEMBL4601': 560, 'CHEMBL5147': 561, 'CHEMBL4134': 562, 'CHEMBL4602': 563, 'CHEMBL3817': 564, 'CHEMBL2094115': 565, 'CHEMBL3562162': 566, 'CHEMBL4879436': 567, 'CHEMBL1075132': 568, 'CHEMBL1075323': 569, 'CHEMBL4915': 570, 'CHEMBL1671608': 571, 'CHEMBL5475': 572, 'CHEMBL2107': 573, 'CHEMBL1250348': 574, 'CHEMBL3575': 575, 'CHEMBL5814': 576, 'CHEMBL3869': 577, 'CHEMBL6159': 578, 'CHEMBL3883324': 579, 'CHEMBL3430886': 580, 'CHEMBL2664': 581, 'CHEMBL1795129': 582, 'CHEMBL4096': 583, 'CHEMBL4848': 584, 'CHEMBL5294': 585, 'CHEMBL2167': 586, 'CHEMBL2034': 587, 'CHEMBL5169270': 588, 'CHEMBL5523': 589, 'CHEMBL3885542': 590, 'CHEMBL6158': 591, 'CHEMBL5028': 592, 'CHEMBL1293227': 593, 'CHEMBL5288': 594, 'CHEMBL1293287': 595, 'CHEMBL4581': 596, 'CHEMBL2094127': 597, 'CHEMBL2094126': 598, 'CHEMBL2095186': 599, 'CHEMBL1795185': 600, 'CHEMBL4295749': 601, 'CHEMBL2392': 602, 'CHEMBL1952': 603, 'CHEMBL1293258': 604, 'CHEMBL2390810': 605, 'CHEMBL5804': 606, 'CHEMBL5936': 607, 'CHEMBL3885615': 608, 'CHEMBL3286061': 609, 'CHEMBL6101': 610, 'CHEMBL4295922': 611, 'CHEMBL3902': 612, 'CHEMBL4106131': 613, 'CHEMBL3559682': 614, 'CHEMBL3038491': 615, 'CHEMBL3301390': 616, 'CHEMBL2176772': 617, 'CHEMBL2331064': 618, 'CHEMBL3396': 619, 'CHEMBL5684': 620, 'CHEMBL3108646': 621, 'CHEMBL3885612': 622, 'CHEMBL5772': 623, 'CHEMBL3492': 624, 'CHEMBL4208': 625, 'CHEMBL2616': 626, 'CHEMBL3638349': 627, 'CHEMBL1997': 628, 'CHEMBL2221344': 629, 'CHEMBL1961792': 630, 'CHEMBL4630760': 631, 'CHEMBL4680051': 632, 'CHEMBL2111448': 633, 'CHEMBL2518': 634, 'CHEMBL4454': 635, 'CHEMBL4223': 636, 'CHEMBL5874': 637, 'CHEMBL5654': 638, 'CHEMBL4162': 639, 'CHEMBL5500': 640, 'CHEMBL5810': 641, 'CHEMBL4523285': 642, 'CHEMBL1977': 643, 'CHEMBL5924': 644, 'CHEMBL2864': 645, 'CHEMBL6142': 646, 'CHEMBL4295895': 647, 'CHEMBL4458': 648, 'CHEMBL4801': 649, 'CHEMBL1907606': 650, 'CHEMBL3883293': 651, 'CHEMBL4296022': 652, 'CHEMBL2069164': 653, 'CHEMBL2706': 654, 'CHEMBL1782': 655, 'CHEMBL2378': 656, 'CHEMBL3301391': 657, 'CHEMBL1949': 658, 'CHEMBL4523137': 659, 'CHEMBL5291610': 660, 'CHEMBL5169264': 661, 'CHEMBL3430887': 662, 'CHEMBL1795176': 663, 'CHEMBL2493': 664, 'CHEMBL3038467': 665, 'CHEMBL2099': 666, 'CHEMBL5954': 667, 'CHEMBL5520': 668, 'CHEMBL5840': 669, 'CHEMBL3234': 670, 'CHEMBL3108660': 671, 'CHEMBL4766': 672, 'CHEMBL5019': 673, 'CHEMBL5169149': 674, 'CHEMBL3298': 675, 'CHEMBL3404': 676, 'CHEMBL5169115': 677, 'CHEMBL1781868': 678, 'CHEMBL4101': 679, 'CHEMBL3392950': 680, 'CHEMBL4459': 681, 'CHEMBL3137268': 682, 'CHEMBL3885588': 683, 'CHEMBL1804': 684, 'CHEMBL2034807': 685, 'CHEMBL3217393': 686, 'CHEMBL4680043': 687, 'CHEMBL6095': 688, 'CHEMBL4523620': 689, 'CHEMBL4879409': 690, 'CHEMBL3582': 691, 'CHEMBL6173': 692, 'CHEMBL3883287': 693, 'CHEMBL5347': 694, 'CHEMBL3032': 695, 'CHEMBL2918': 696, 'CHEMBL5969': 697, 'CHEMBL2688': 698, 'CHEMBL4523673': 699, 'CHEMBL2157853': 700, 'CHEMBL3407323': 701, 'CHEMBL3094': 702, 'CHEMBL2111463': 703, 'CHEMBL2364163': 704, 'CHEMBL2075': 705, 'CHEMBL2593': 706, 'CHEMBL4680050': 707, 'CHEMBL3038484': 708, 'CHEMBL5427': 709, 'CHEMBL4888450': 710, 'CHEMBL6005': 711, 'CHEMBL3167': 712, 'CHEMBL4520': 713, 'CHEMBL3885550': 714, 'CHEMBL2096667': 715, 'CHEMBL2093867': 716, 'CHEMBL2163182': 717, 'CHEMBL2061': 718, 'CHEMBL3885613': 719, 'CHEMBL1795178': 720, 'CHEMBL2176853': 721, 'CHEMBL4523986': 722, 'CHEMBL5467': 723, 'CHEMBL5455': 724, 'CHEMBL4802034': 725, 'CHEMBL6029': 726, 'CHEMBL2889': 727, 'CHEMBL2845': 728, 'CHEMBL3351199': 729, 'CHEMBL5661': 730, 'CHEMBL2938': 731, 'CHEMBL5169137': 732, 'CHEMBL3883285': 733, 'CHEMBL5169156': 734, 'CHEMBL1163106': 735, 'CHEMBL1613743': 736, 'CHEMBL3085622': 737, 'CHEMBL2424510': 738, 'CHEMBL3509589': 739, 'CHEMBL5838': 740, 'CHEMBL2693': 741, 'CHEMBL3396943': 742, 'CHEMBL3028': 743, 'CHEMBL5705': 744, 'CHEMBL2111431': 745, 'CHEMBL4630723': 746, 'CHEMBL3885553': 747, 'CHEMBL4296115': 748, 'CHEMBL4791': 749, 'CHEMBL3621036': 750, 'CHEMBL5169127': 751, 'CHEMBL5169265': 752, 'CHEMBL3038475': 753, 'CHEMBL2077': 754, 'CHEMBL4888449': 755, 'CHEMBL3402': 756, 'CHEMBL5291951': 757, 'CHEMBL1944': 758, 'CHEMBL5533': 759, 'CHEMBL2558': 760, 'CHEMBL2111472': 761, 'CHEMBL4888444': 762, 'CHEMBL1795098': 763, 'CHEMBL3430877': 764, 'CHEMBL2334': 765, 'CHEMBL3509601': 766, 'CHEMBL4445': 767, 'CHEMBL4523633': 768, 'CHEMBL2095226': 769, 'CHEMBL206': 770, 'CHEMBL242': 771, 'CHEMBL1293229': 772, 'CHEMBL2429709': 773, 'CHEMBL2506': 774, 'CHEMBL4295939': 775, 'CHEMBL3883330': 776, 'CHEMBL3258': 777, 'CHEMBL4739705': 778, 'CHEMBL3863': 779, 'CHEMBL4295894': 780, 'CHEMBL5988': 781, 'CHEMBL5169272': 782, 'CHEMBL5169271': 783, 'CHEMBL3708585': 784, 'CHEMBL5291978': 785, 'CHEMBL5021': 786, 'CHEMBL1781870': 787, 'CHEMBL3976': 788, 'CHEMBL5501': 789, 'CHEMBL1992': 790, 'CHEMBL4739683': 791, 'CHEMBL3774298': 792, 'CHEMBL3421525': 793, 'CHEMBL3721311': 794, 'CHEMBL3201': 795, 'CHEMBL3588738': 796, 'CHEMBL1741176': 797, 'CHEMBL274': 798, 'CHEMBL4388': 799, 'CHEMBL1163126': 800, 'CHEMBL2096665': 801, 'CHEMBL2062349': 802, 'CHEMBL4523469': 803, 'CHEMBL4296098': 804, 'CHEMBL5164': 805, 'CHEMBL3521': 806, 'CHEMBL3137286': 807, 'CHEMBL3918': 808, 'CHEMBL1795140': 809, 'CHEMBL1697657': 810, 'CHEMBL2094253': 811, 'CHEMBL4802036': 812, 'CHEMBL2120': 813, 'CHEMBL4234': 814, 'CHEMBL4106152': 815, 'CHEMBL4879458': 816, 'CHEMBL3137291': 817, 'CHEMBL5837': 818, 'CHEMBL4198': 819, 'CHEMBL1649059': 820, 'CHEMBL1856': 821, 'CHEMBL2189139': 822, 'CHEMBL4158': 823, 'CHEMBL3430897': 824, 'CHEMBL4106164': 825, 'CHEMBL4607': 826, 'CHEMBL5293': 827, 'CHEMBL1792': 828, 'CHEMBL2111345': 829, 'CHEMBL4899': 830, 'CHEMBL2073661': 831, 'CHEMBL5983': 832, 'CHEMBL5976': 833, 'CHEMBL4163': 834, 'CHEMBL2176839': 835, 'CHEMBL3251': 836, 'CHEMBL3776': 837, 'CHEMBL3959': 838, 'CHEMBL6014': 839, 'CHEMBL3885516': 840, 'CHEMBL5169269': 841, 'CHEMBL2176846': 842, 'CHEMBL5318': 843, 'CHEMBL4106180': 844, 'CHEMBL2111289': 845, 'CHEMBL4954': 846, 'CHEMBL4879432': 847, 'CHEMBL4879435': 848, 'CHEMBL3038515': 849, 'CHEMBL3253': 850, 'CHEMBL272': 851, 'CHEMBL4360': 852, 'CHEMBL3987': 853, 'CHEMBL5364': 854, 'CHEMBL5721': 855, 'CHEMBL3243': 856, 'CHEMBL4802035': 857, 'CHEMBL5650': 858, 'CHEMBL6088': 859, 'CHEMBL5291509': 860, 'CHEMBL3873': 861, 'CHEMBL2189153': 862, 'CHEMBL5985': 863, 'CHEMBL5605': 864, 'CHEMBL5291568': 865, 'CHEMBL2010634': 866, 'CHEMBL3924': 867, 'CHEMBL2468': 868, 'CHEMBL1615383': 869, 'CHEMBL5169268': 870, 'CHEMBL4879514': 871, 'CHEMBL3544': 872, 'CHEMBL3784904': 873, 'CHEMBL2364164': 874, 'CHEMBL5716': 875, 'CHEMBL3883298': 876, 'CHEMBL1741191': 877, 'CHEMBL4295658': 878, 'CHEMBL5169200': 879, 'CHEMBL2898': 880, 'CHEMBL1615381': 881, 'CHEMBL3174': 882, 'CHEMBL2010624': 883, 'CHEMBL3885524': 884, 'CHEMBL5602': 885, 'CHEMBL5550': 886, 'CHEMBL2079845': 887, 'CHEMBL4630820': 888, 'CHEMBL5575': 889, 'CHEMBL3893': 890, 'CHEMBL3286079': 891, 'CHEMBL3687': 892, 'CHEMBL2442': 893, 'CHEMBL4884': 894, 'CHEMBL3131': 895, 'CHEMBL2273': 896, 'CHEMBL5037': 897, 'CHEMBL5615': 898, 'CHEMBL4295757': 899, 'CHEMBL1275223': 900, 'CHEMBL5516': 901, 'CHEMBL2896': 902, 'CHEMBL1795123': 903, 'CHEMBL2597': 904, 'CHEMBL2111458': 905, 'CHEMBL1293314': 906, 'CHEMBL5169275': 907, 'CHEMBL5975': 908, 'CHEMBL3763008': 909, 'CHEMBL208': 910, 'CHEMBL5169150': 911, 'CHEMBL2169735': 912, 'CHEMBL3038468': 913, 'CHEMBL3301386': 914, 'CHEMBL3107': 915, 'CHEMBL3885552': 916, 'CHEMBL1944496': 917, 'CHEMBL3039': 918, 'CHEMBL3301392': 919, 'CHEMBL3038492': 920, 'CHEMBL5555': 921, 'CHEMBL2473': 922, 'CHEMBL1075169': 923, 'CHEMBL1075105': 924, 'CHEMBL3038494': 925, 'CHEMBL2047': 926, 'CHEMBL5291558': 927, 'CHEMBL1075315': 928, 'CHEMBL4295798': 929, 'CHEMBL5291975': 930, 'CHEMBL3831205': 931, 'CHEMBL3885560': 932, 'CHEMBL2189114': 933, 'CHEMBL2189113': 934, 'CHEMBL2189112': 935, 'CHEMBL1795118': 936, 'CHEMBL3885617': 937, 'CHEMBL3792271': 938, 'CHEMBL4295787': 939, 'CHEMBL3627581': 940, 'CHEMBL2337': 941, 'CHEMBL271': 942, 'CHEMBL5982': 943, 'CHEMBL2823': 944, 'CHEMBL3627585': 945, 'CHEMBL6093': 946, 'CHEMBL4699': 947, 'CHEMBL4888461': 948, 'CHEMBL4630857': 949, 'CHEMBL5291557': 950, 'CHEMBL2193': 951, 'CHEMBL3514': 952, 'CHEMBL5291553': 953, 'CHEMBL2108': 954, 'CHEMBL5320': 955, 'CHEMBL1961788': 956, 'CHEMBL2111435': 957, 'CHEMBL2363049': 958, 'CHEMBL3712894': 959, 'CHEMBL4296088': 960, 'CHEMBL4523621': 961, 'CHEMBL5917': 962, 'CHEMBL3972': 963, 'CHEMBL2364675': 964, 'CHEMBL1938212': 965, 'CHEMBL5291609': 966, 'CHEMBL4523287': 967, 'CHEMBL4523598': 968, 'CHEMBL3758068': 969, 'CHEMBL4896': 970, 'CHEMBL2221341': 971, 'CHEMBL4437': 972, 'CHEMBL5291564': 973, 'CHEMBL5912': 974, 'CHEMBL2598': 975, 'CHEMBL5052': 976, 'CHEMBL4516': 977, 'CHEMBL1075166': 978, 'CHEMBL5519': 979, 'CHEMBL4106161': 980, 'CHEMBL4523637': 981, 'CHEMBL1845': 982, 'CHEMBL5169266': 983, 'CHEMBL3713006': 984, 'CHEMBL4295759': 985, 'CHEMBL3308910': 986, 'CHEMBL3589': 987, 'CHEMBL3885643': 988, 'CHEMBL4662938': 989, 'CHEMBL4879446': 990, 'CHEMBL4881': 991, 'CHEMBL5960': 992, 'CHEMBL4680046': 993, 'CHEMBL4680045': 994, 'CHEMBL2062350': 995, 'CHEMBL3964': 996, 'CHEMBL5750': 997, 'CHEMBL3774299': 998, 'CHEMBL5291972': 999, 'CHEMBL2111407': 1000, 'CHEMBL5870': 1001, 'CHEMBL3610': 1002, 'CHEMBL5169124': 1003, 'CHEMBL3885562': 1004, 'CHEMBL2406900': 1005, 'CHEMBL4296108': 1006, 'CHEMBL5291974': 1007, 'CHEMBL4523987': 1008, 'CHEMBL4523631': 1009, 'CHEMBL5291599': 1010, 'CHEMBL4106129': 1011, 'CHEMBL2095180': 1012, 'CHEMBL3038451': 1013, 'CHEMBL3038452': 1014, 'CHEMBL3038453': 1015, 'CHEMBL4106162': 1016, 'CHEMBL3885504': 1017, 'CHEMBL4106159': 1018, 'CHEMBL3038456': 1019, 'CHEMBL4106158': 1020, 'CHEMBL3038457': 1021, 'CHEMBL3038455': 1022}\n"
     ]
    }
   ],
   "source": [
    "print(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "Batch number: 0.0\n",
      "Batch number: 1.0\n",
      "Batch number: 2.0\n",
      "Batch number: 3.0\n",
      "Batch number: 4.0\n",
      "Batch number: 5.0\n",
      "Batch number: 6.0\n",
      "Batch number: 7.0\n",
      "Batch number: 8.0\n",
      "Error\n",
      "Batch number: 9.0\n",
      "Batch number: 10.0\n",
      "Batch number: 11.0\n",
      "Error\n",
      "Batch number: 12.0\n",
      "Batch number: 13.0\n",
      "Batch number: 14.0\n",
      "Batch number: 15.0\n",
      "Batch number: 16.0\n",
      "Batch number: 17.0\n",
      "Batch number: 18.0\n",
      "Batch number: 19.0\n",
      "Batch number: 20.0\n",
      "Batch number: 21.0\n",
      "Batch number: 22.0\n",
      "Batch number: 23.0\n",
      "Batch number: 24.0\n",
      "Error\n",
      "Batch number: 25.0\n",
      "Batch number: 26.0\n",
      "Batch number: 27.0\n",
      "Error\n",
      "Batch number: 28.0\n",
      "Batch number: 29.0\n",
      "Batch number: 30.0\n",
      "Error\n",
      "Batch number: 31.0\n",
      "Batch number: 32.0\n",
      "Batch number: 33.0\n",
      "Error\n",
      "Batch number: 34.0\n",
      "Batch number: 35.0\n",
      "Batch number: 36.0\n",
      "Error\n",
      "Batch number: 37.0\n",
      "Batch number: 38.0\n",
      "Batch number: 39.0\n",
      "Batch number: 40.0\n",
      "Batch number: 41.0\n",
      "Batch number: 42.0\n",
      "Batch number: 43.0\n",
      "Error\n",
      "Batch number: 44.0\n",
      "Batch number: 45.0\n",
      "Batch number: 46.0\n",
      "Batch number: 47.0\n",
      "Batch number: 48.0\n",
      "Error\n",
      "Batch number: 49.0\n",
      "Batch number: 50.0\n",
      "Batch number: 51.0\n",
      "Error\n",
      "Batch number: 52.0\n",
      "Batch number: 53.0\n",
      "Batch number: 54.0\n",
      "Batch number: 55.0\n",
      "Batch number: 56.0\n",
      "Batch number: 57.0\n",
      "Batch number: 58.0\n",
      "Batch number: 59.0\n",
      "Batch number: 60.0\n",
      "Batch number: 61.0\n",
      "Batch number: 62.0\n",
      "Batch number: 63.0\n",
      "Batch number: 64.0\n",
      "Batch number: 65.0\n",
      "Batch number: 66.0\n",
      "Error\n",
      "Batch number: 67.0\n",
      "Batch number: 68.0\n",
      "Batch number: 69.0\n",
      "Batch number: 70.0\n",
      "Batch number: 71.0\n",
      "Batch number: 72.0\n",
      "Batch number: 73.0\n",
      "Error\n",
      "Batch number: 74.0\n",
      "Batch number: 75.0\n",
      "Batch number: 76.0\n",
      "Batch number: 77.0\n",
      "Batch number: 78.0\n",
      "Batch number: 79.0\n",
      "Batch number: 80.0\n",
      "Batch number: 81.0\n",
      "Batch number: 82.0\n",
      "Batch number: 83.0\n",
      "Batch number: 84.0\n",
      "Batch number: 85.0\n",
      "Batch number: 86.0\n",
      "Error\n",
      "Batch number: 87.0\n",
      "Batch number: 88.0\n",
      "Batch number: 89.0\n",
      "Batch number: 90.0\n",
      "Batch number: 91.0\n",
      "Batch number: 92.0\n",
      "Error\n",
      "Batch number: 93.0\n",
      "Batch number: 94.0\n",
      "Batch number: 95.0\n",
      "Error\n",
      "Batch number: 96.0\n",
      "Batch number: 97.0\n",
      "Batch number: 98.0\n",
      "Batch number: 99.0\n",
      "Batch number: 100.0\n",
      "Batch number: 101.0\n",
      "Batch number: 102.0\n",
      "Batch number: 103.0\n",
      "Batch number: 104.0\n",
      "Error\n",
      "Batch number: 105.0\n",
      "Batch number: 106.0\n",
      "Error\n",
      "Batch number: 107.0\n",
      "Error\n",
      "Batch number: 108.0\n",
      "Batch number: 109.0\n",
      "An error occurred: HTTPSConnectionPool(host='www.ebi.ac.uk', port=443): Read timed out.\n",
      "Batch number: 110.0\n",
      "Connection timed out. Please check your network or try again later.\n",
      "Batch number: 111.0\n",
      "Batch number: 112.0\n",
      "Batch number: 113.0\n",
      "Batch number: 114.0\n",
      "Batch number: 115.0\n",
      "Batch number: 116.0\n",
      "Batch number: 117.0\n",
      "Batch number: 118.0\n",
      "Batch number: 119.0\n",
      "Batch number: 120.0\n",
      "Batch number: 121.0\n",
      "Error\n",
      "Batch number: 122.0\n",
      "Batch number: 123.0\n",
      "Batch number: 124.0\n",
      "Error\n",
      "Batch number: 125.0\n",
      "Batch number: 126.0\n",
      "Error\n",
      "Batch number: 127.0\n",
      "Batch number: 128.0\n",
      "Batch number: 129.0\n",
      "Batch number: 130.0\n",
      "Batch number: 131.0\n",
      "Batch number: 132.0\n",
      "Batch number: 133.0\n",
      "Batch number: 134.0\n",
      "Batch number: 135.0\n",
      "Batch number: 136.0\n",
      "Batch number: 137.0\n",
      "Batch number: 138.0\n",
      "Batch number: 139.0\n",
      "Batch number: 140.0\n",
      "Batch number: 141.0\n",
      "Batch number: 142.0\n",
      "Batch number: 143.0\n",
      "Batch number: 144.0\n",
      "Batch number: 145.0\n",
      "Batch number: 146.0\n",
      "Error\n",
      "Batch number: 147.0\n",
      "Batch number: 148.0\n",
      "Batch number: 149.0\n",
      "Batch number: 150.0\n",
      "Batch number: 151.0\n",
      "Batch number: 152.0\n",
      "Batch number: 153.0\n",
      "Batch number: 154.0\n",
      "Batch number: 155.0\n",
      "Batch number: 156.0\n",
      "Batch number: 157.0\n",
      "Batch number: 158.0\n",
      "Batch number: 159.0\n",
      "Batch number: 160.0\n",
      "Batch number: 161.0\n",
      "Batch number: 162.0\n",
      "Error\n",
      "Batch number: 163.0\n",
      "Batch number: 164.0\n",
      "Batch number: 165.0\n",
      "Error\n",
      "Batch number: 166.0\n",
      "Batch number: 167.0\n",
      "Batch number: 168.0\n",
      "Error\n",
      "Batch number: 169.0\n",
      "Batch number: 170.0\n",
      "Batch number: 171.0\n",
      "Batch number: 172.0\n",
      "Error\n",
      "Batch number: 173.0\n",
      "Batch number: 174.0\n",
      "Batch number: 175.0\n",
      "Batch number: 176.0\n",
      "Batch number: 177.0\n",
      "Batch number: 178.0\n",
      "Batch number: 179.0\n",
      "Error\n",
      "Batch number: 180.0\n",
      "Batch number: 181.0\n",
      "Batch number: 182.0\n",
      "Batch number: 183.0\n",
      "Batch number: 184.0\n",
      "Batch number: 185.0\n",
      "Batch number: 186.0\n"
     ]
    }
   ],
   "source": [
    "def fetch_smiles_set(chembl_ids):\n",
    "    url = f\"https://www.ebi.ac.uk/chembl/api/data/molecule/set/{';'.join(chembl_ids)}\"\n",
    "    \n",
    "    # Add the 'Accept' header to explicitly request JSON format\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Set a timeout of 10 seconds (you can adjust this as needed)\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()  # Parse JSON response\n",
    "                smiles_dict = {}\n",
    "                for molecule in data.get('molecules', []):\n",
    "                    smiles = molecule.get('molecule_structures', {}).get('canonical_smiles', None)\n",
    "                    chembl_id = molecule['molecule_chembl_id']\n",
    "                    smiles_dict[chembl_id] = smiles\n",
    "                return smiles_dict\n",
    "            except:\n",
    "                print(\"Error\")\n",
    "                return {}\n",
    "        else:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            return {}\n",
    "    except requests.exceptions.ConnectTimeout:\n",
    "        print(\"Connection timed out. Please check your network or try again later.\")\n",
    "        return {}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "def fetch_all_smiles(df, batch_size=100):\n",
    "    unique_chembl_ids = df['compound_chembl_id'].unique()\n",
    "    smiles_dict = {}\n",
    "\n",
    "    # Fetch SMILES in batches using the /set/ endpoint\n",
    "    for i in range(0, len(unique_chembl_ids), batch_size):\n",
    "        batch_ids = unique_chembl_ids[i:i+batch_size]\n",
    "        batch_smiles = fetch_smiles_set(batch_ids)\n",
    "        smiles_dict.update(batch_smiles)\n",
    "        print(f\"Batch number: {i/batch_size}\")\n",
    "\n",
    "    return smiles_dict\n",
    "\n",
    "# Example usage\n",
    "smiles_dict = fetch_all_smiles(df, batch_size=250)\n",
    "\n",
    "df['compound_smiles'] = df['compound_chembl_id'].map(smiles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_chembl_inhibit_drug_target_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "Batch number: 0.0\n",
      "Error\n",
      "Batch number: 1.0\n",
      "Error\n",
      "Batch number: 2.0\n",
      "Batch number: 3.0\n",
      "Batch number: 4.0\n",
      "Error\n",
      "Batch number: 5.0\n",
      "Error\n",
      "Batch number: 6.0\n",
      "Batch number: 7.0\n",
      "Batch number: 8.0\n",
      "Error\n",
      "Batch number: 9.0\n",
      "Error\n",
      "Batch number: 10.0\n",
      "Batch number: 11.0\n",
      "Batch number: 12.0\n",
      "Error\n",
      "Batch number: 13.0\n",
      "Batch number: 14.0\n",
      "Error\n",
      "Batch number: 15.0\n",
      "Error\n",
      "Batch number: 16.0\n",
      "Batch number: 17.0\n",
      "Error\n",
      "Batch number: 18.0\n",
      "Batch number: 19.0\n",
      "Error\n",
      "Batch number: 20.0\n",
      "Batch number: 21.0\n",
      "Error\n",
      "Batch number: 22.0\n",
      "Batch number: 23.0\n",
      "Batch number: 24.0\n",
      "Error\n",
      "Batch number: 25.0\n",
      "Batch number: 26.0\n",
      "Error\n",
      "Batch number: 27.0\n",
      "Batch number: 28.0\n",
      "Error\n",
      "Batch number: 29.0\n",
      "Error\n",
      "Batch number: 30.0\n",
      "Batch number: 31.0\n",
      "Error\n",
      "Batch number: 32.0\n",
      "Batch number: 33.0\n",
      "Error\n",
      "Batch number: 34.0\n",
      "Batch number: 35.0\n",
      "Batch number: 36.0\n",
      "Error\n",
      "Batch number: 37.0\n",
      "Error\n",
      "Batch number: 38.0\n",
      "Batch number: 39.0\n",
      "Batch number: 40.0\n",
      "Error\n",
      "Batch number: 41.0\n",
      "Error\n",
      "Batch number: 42.0\n",
      "Batch number: 43.0\n",
      "Error\n",
      "Batch number: 44.0\n",
      "Error\n",
      "Batch number: 45.0\n",
      "Batch number: 46.0\n",
      "Error\n",
      "Batch number: 47.0\n",
      "Error\n",
      "Batch number: 48.0\n",
      "Error\n",
      "Batch number: 49.0\n",
      "Error\n",
      "Batch number: 50.0\n",
      "Error\n",
      "Batch number: 51.0\n",
      "Batch number: 52.0\n",
      "Error\n",
      "Batch number: 53.0\n",
      "Batch number: 54.0\n",
      "Error\n",
      "Batch number: 55.0\n",
      "Batch number: 56.0\n",
      "Batch number: 57.0\n",
      "Error\n",
      "Batch number: 58.0\n",
      "Batch number: 59.0\n",
      "Batch number: 60.0\n",
      "Error\n",
      "Batch number: 61.0\n",
      "Error\n",
      "Batch number: 62.0\n",
      "Batch number: 63.0\n",
      "Batch number: 64.0\n",
      "Error\n",
      "Batch number: 65.0\n",
      "Error\n",
      "Batch number: 66.0\n",
      "Batch number: 67.0\n"
     ]
    }
   ],
   "source": [
    "nan_smiles_df = df[df['compound_smiles'].isna()]\n",
    "print(len(nan_smiles_df))\n",
    "\n",
    "missing_smiles_dict = fetch_all_smiles(nan_smiles_df, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_smiles(row):\n",
    "    if pd.isna(row['compound_smiles']):  # Check if the current value is NaN\n",
    "        chembl_id = row['compound_chembl_id']  # Get the ChEMBL ID from the row\n",
    "        return missing_smiles_dict.get(chembl_id, row['compound_smiles'])  # Get the SMILES from the dict or keep NaN if not found\n",
    "    else:\n",
    "        return row['compound_smiles']  # Return the existing SMILES if it's not NaN\n",
    "\n",
    "# Apply the function to fill missing SMILES\n",
    "df['compound_smiles'] = df.apply(fill_missing_smiles, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "nan_smiles_df = df[df['compound_smiles'].isna()]\n",
    "print(len(nan_smiles_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_chembl_inhibit_drug_target_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14280\\4075605751.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Handle missing molecule cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Get the list of smiles from compound_map and df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0msmiles_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compound_chembl_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcompound\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compound_smiles'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcompound\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcompounds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Extract compound features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mcompound_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_compound_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14280\\4075605751.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mextract_compound_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdFingerprintGenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetMorganGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfpSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msmiles\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msmiles_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\chakr\\Desktop\\BTP\\env\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4089\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4091\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4092\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4093\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4095\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4096\u001b[0m         \u001b[1;31m# We interpret tuples as collections only for non-MultiIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\chakr\\Desktop\\BTP\\env\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4151\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4154\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4155\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Function to extract molecular features using RDKit\n",
    "def extract_compound_features(smiles_list):\n",
    "    features = []\n",
    "    generator = rdFingerprintGenerator.GetMorganGenerator(radius=2,fpSize=1024)\n",
    "    for smiles in smiles_list:\n",
    "        if isinstance(smiles, float) and math.isnan(smiles):\n",
    "            features.append([0] * 1024)\n",
    "            continue\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            # Example: Using molecular weight and LogP as features\n",
    "            fingerprint = generator.GetFingerprint(mol)\n",
    "            fingerprint_list = list(fingerprint)  # Convert to list if needed\n",
    "            features.append(fingerprint_list)\n",
    "        else:\n",
    "            features.append([0] * 1024)  # Handle missing molecule cases\n",
    "    return torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "# Get the list of smiles from compound_map and df\n",
    "smiles_list = [df[df['compound_chembl_id'] == compound].iloc[0]['compound_smiles'] for compound in compounds]\n",
    "\n",
    "# Extract compound features\n",
    "compound_features = extract_compound_features(smiles_list)\n",
    "\n",
    "# Add compound features to the graph\n",
    "data['compound'].x = compound_features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46660, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(data['compound'].x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data['compound'].x\n",
    "torch.save(data['compound'].x, 'compound_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data['compound'].x\n",
    "data['compound'].x = torch.load('compound_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(data['compound'].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 1\n",
      "Processed batch number: 2\n",
      "Processed batch number: 3\n",
      "Processed batch number: 4\n",
      "Processed batch number: 5\n"
     ]
    }
   ],
   "source": [
    "def fetch_uniprot_set(chembl_ids):\n",
    "    url = f\"https://www.ebi.ac.uk/chembl/api/data/target/set/{';'.join(chembl_ids)}\"\n",
    "    \n",
    "    # Add the 'Accept' header to explicitly request JSON format\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Set a timeout of 10 seconds (you can adjust this as needed)\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()  # Parse JSON response\n",
    "                uniprot_dict = {}\n",
    "                for target in data.get('targets', []):\n",
    "                    target_components = target.get('target_components', [])\n",
    "                    for component in target_components:\n",
    "                        for xref in component.get('target_component_xrefs', []):\n",
    "                            if xref.get('xref_src_db') == 'UniProt':\n",
    "                                chembl_id = target['target_chembl_id']\n",
    "                                uniprot_id = xref.get('xref_id')\n",
    "                                uniprot_dict[chembl_id] = uniprot_id\n",
    "                return uniprot_dict\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing response: {e}\")\n",
    "                return {}\n",
    "        else:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            return {}\n",
    "    except requests.exceptions.ConnectTimeout:\n",
    "        print(\"Connection timed out. Please check your network or try again later.\")\n",
    "        return {}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Function to fetch all UniProt IDs in batches, similar to fetch_all_smiles\n",
    "def fetch_all_uniprots(df, batch_size=100):\n",
    "    unique_chembl_ids = df['target_chembl_id'].unique()  # Assuming 'target_chembl_id' column exists\n",
    "    uniprot_dict = {}\n",
    "\n",
    "    # Fetch UniProt IDs in batches using the /target/set/ endpoint\n",
    "    for i in range(0, len(unique_chembl_ids), batch_size):\n",
    "        batch_ids = unique_chembl_ids[i:i+batch_size]\n",
    "        batch_uniprots = fetch_uniprot_set(batch_ids)\n",
    "        uniprot_dict.update(batch_uniprots)\n",
    "        print(f\"Processed batch number: {i // batch_size + 1}\")\n",
    "\n",
    "    return uniprot_dict\n",
    "\n",
    "# Example usage\n",
    "uniprot_dict = fetch_all_uniprots(df, batch_size=250)\n",
    "\n",
    "# Map the fetched UniProt IDs back to the original DataFrame\n",
    "df['target_uniprot_id'] = df['target_chembl_id'].map(uniprot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_chembl_inhibit_drug_target_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the Prot5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "print(\"Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = T5Model.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\").half().to(device)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2123\n",
      "Processing uniprot id : Q59GZ4 number 20\n",
      "Processing uniprot id : Q9BZX1 number 40\n",
      "Processing uniprot id : Q96RG3 number 60\n",
      "Processing uniprot id : Q8N7V3 number 80\n",
      "Processing uniprot id : Q9UMG5 number 100\n",
      "Processing uniprot id : Q9BVY6 number 120\n",
      "Processing uniprot id : Q9Y618 number 140\n",
      "Processing uniprot id : Q9NYL2 number 160\n",
      "Processing uniprot id : Q5T489 number 180\n",
      "Processing uniprot id : Q9UCC0 number 200\n",
      "Processing uniprot id : Q9HAR0 number 220\n",
      "Processing uniprot id : Q9UHD2 number 240\n",
      "Processing uniprot id : Q9P0B8 number 260\n",
      "Processing uniprot id : Q9Y265 number 280\n",
      "Processing uniprot id : Q2M1P8 number 300\n",
      "Processing uniprot id : Q9UQ96 number 320\n",
      "Processing uniprot id : Q99957 number 340\n",
      "Processing uniprot id : Q969U4 number 360\n",
      "Processing uniprot id : Q5T7T8 number 380\n",
      "Processing uniprot id : Q96KA8 number 400\n",
      "Processing uniprot id : Q9HD26 number 420\n",
      "Processing uniprot id : Q96CY8 number 440\n",
      "Processing uniprot id : Q9Y616 number 460\n",
      "Processing uniprot id : Q68DZ3 number 480\n",
      "Processing uniprot id : Q8NDH1 number 500\n",
      "Processing uniprot id : Q9Y3S1 number 520\n",
      "Processing uniprot id : Q9UQ95 number 540\n",
      "Processing uniprot id : Q6N0A4 number 560\n",
      "Processing uniprot id : Q9UPH9 number 580\n",
      "Processing uniprot id : Q53S28 number 600\n",
      "Processing uniprot id : Q9H2F1 number 620\n",
      "Processing uniprot id : Q9NRH9 number 640\n",
      "Processing uniprot id : Q9UG86 number 660\n",
      "Processing uniprot id : Q9UHG4 number 680\n",
      "Processing uniprot id : Q5SQI4 number 700\n",
      "Processing uniprot id : Q9NV40 number 720\n",
      "Processing uniprot id : Q9UPA4 number 740\n",
      "Processing uniprot id : Q5VW80 number 760\n",
      "Processing uniprot id : Q969L6 number 780\n",
      "Processing uniprot id : Q9GZQ8 number 800\n",
      "Processing uniprot id : Q9BZL0 number 820\n",
      "Processing uniprot id : Q9UCS6 number 840\n",
      "Processing uniprot id : Q9UDC1 number 860\n",
      "Processing uniprot id : Q9H241 number 880\n",
      "Processing uniprot id : Q9UKM3 number 900\n",
      "Processing uniprot id : P09683 number 920\n",
      "Processing uniprot id : P08922 number 940\n",
      "Error fetching sequence for nan\n",
      "Processing uniprot id : P60484 number 960\n",
      "Processing uniprot id : Q99728 number 980\n",
      "Processing uniprot id : P04198 number 1000\n",
      "Processing uniprot id : P03971 number 1020\n",
      "Processing uniprot id : P53365 number 1040\n",
      "Processing uniprot id : Q15910 number 1060\n",
      "Processing uniprot id : P01730 number 1080\n",
      "Processing uniprot id : P10588 number 1100\n",
      "Processing uniprot id : P22083 number 1120\n",
      "Processing uniprot id : Q9NPI1 number 1140\n",
      "Processing uniprot id : Q02080 number 1160\n",
      "Processing uniprot id : Q8N163 number 1180\n",
      "Processing uniprot id : P09619 number 1200\n",
      "Processing uniprot id : P08727 number 1220\n",
      "Processing uniprot id : Q5JR59 number 1240\n",
      "Processing uniprot id : P61586 number 1260\n",
      "Processing uniprot id : F5H6H0 number 1280\n",
      "Processing uniprot id : Q15532 number 1300\n",
      "Processing uniprot id : P35680 number 1320\n",
      "Processing uniprot id : P01011 number 1340\n",
      "Processing uniprot id : Q53GG5 number 1360\n",
      "Processing uniprot id : P01135 number 1380\n",
      "Processing uniprot id : Q8NFG4 number 1400\n",
      "Processing uniprot id : P05164 number 1420\n",
      "Processing uniprot id : P30153 number 1440\n",
      "Processing uniprot id : Q92733 number 1460\n",
      "Processing uniprot id : Q16548 number 1480\n",
      "Processing uniprot id : Q86U44 number 1500\n",
      "Processing uniprot id : Q9UKF7 number 1520\n",
      "Processing uniprot id : Q9NYF8 number 1540\n",
      "Processing uniprot id : Q14257 number 1560\n",
      "Processing uniprot id : Q9BQE3 number 1580\n",
      "Processing uniprot id : Q9Y266 number 1600\n",
      "Processing uniprot id : P49756 number 1620\n",
      "Processing uniprot id : P11362 number 1640\n",
      "Processing uniprot id : Q9BSJ2 number 1660\n",
      "Processing uniprot id : P30086 number 1680\n",
      "Processing uniprot id : P0C0S5 number 1700\n",
      "Processing uniprot id : Q16637 number 1720\n",
      "Processing uniprot id : Q9Y6Y0 number 1740\n",
      "Processing uniprot id : P57088 number 1760\n",
      "Processing uniprot id : P62854 number 1780\n",
      "Processing uniprot id : Q96L21 number 1800\n",
      "Processing uniprot id : Q96EY1 number 1820\n",
      "Processing uniprot id : P63220 number 1840\n",
      "Processing uniprot id : O14602 number 1860\n",
      "Processing uniprot id : P07919 number 1880\n",
      "Processing uniprot id : Q92485 number 1900\n",
      "Processing uniprot id : P28290 number 1920\n",
      "Processing uniprot id : Q9NQX4 number 1940\n",
      "Processing uniprot id : Q9BT78 number 1960\n",
      "Processing uniprot id : P46459 number 1980\n",
      "Processing uniprot id : Q9P2K3 number 2000\n",
      "Processing uniprot id : Q01650 number 2020\n",
      "Processing uniprot id : Q08050 number 2040\n",
      "Processing uniprot id : P19419 number 2060\n",
      "Processing uniprot id : O75113 number 2080\n",
      "Processing uniprot id : O00220 number 2100\n",
      "Processing uniprot id : Q9NXR1 number 2120\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch protein sequence from UniProt\n",
    "def fetch_uniprot_sequence(uniprot_id):\n",
    "    url = f\"https://www.uniprot.org/uniprot/{uniprot_id}.fasta\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        sequence = ''.join(response.text.splitlines()[1:])  # Skip the FASTA header\n",
    "        return sequence\n",
    "    else:\n",
    "        print(f\"Error fetching sequence for {uniprot_id}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate embeddings for a list of protein sequences\n",
    "def get_protein_embeddings(uniprot_ids):\n",
    "    embeddings_dict = {}  # Dictionary to cache embeddings\n",
    "    num = 0\n",
    "\n",
    "    for uniprot_id in uniprot_ids:\n",
    "        num += 1\n",
    "        if num%20 == 0:\n",
    "            print(f\"Processing uniprot id : {uniprot_id} number {num}\")\n",
    "\n",
    "        try:\n",
    "            sequence = fetch_uniprot_sequence(uniprot_id)  # Fetch the protein sequence\n",
    "            if sequence:\n",
    "                # Tokenize the input sequence\n",
    "                inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "                # Add decoder_input_ids: initialize with the pad token id\n",
    "                decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]]).to(device)\n",
    "\n",
    "                # Forward pass with encoder input and decoder input\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids=inputs['input_ids'], decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "                # Extract embeddings (e.g., from encoder output or mean pooling)\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1)  # Mean pooling for simplicity\n",
    "                embeddings_dict[uniprot_id] = embedding.squeeze(0).cpu().numpy()  # Cache the embedding\n",
    "            else:\n",
    "                embeddings_dict[uniprot_id] = [0] * 1024  # Default zero embedding for missing sequences\n",
    "        except:\n",
    "            print(f\"Error processing uniprot id : {uniprot_id} number {num}\")\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "# Generate the embeddings for the unique UniProt IDs\n",
    "unique_uniprot_ids = protein_map.keys()\n",
    "print(len(unique_uniprot_ids))\n",
    "embeddings_dict = get_protein_embeddings(unique_uniprot_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids = [m_id for m_id in unique_uniprot_ids if m_id not in embeddings_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q9UQ95': array([ 0.1744,  0.2167, -0.1216, ...,  0.1583, -0.0855, -0.5703],\n",
      "      dtype=float16), 'Q9NQ14': array([ 0.1744,  0.2167, -0.1216, ...,  0.1583, -0.0855, -0.5703],\n",
      "      dtype=float16)}\n"
     ]
    }
   ],
   "source": [
    "new_dict = get_protein_embeddings(missing_ids)\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embeddings_dict_merged \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43membeddings_dict\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_dict}\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embeddings_dict_merged))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# embeddings_dict_merged = {**embeddings_dict, **new_dict}\n",
    "# print(len(embeddings_dict_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"embeddings_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "embeddings_dict_merged = pickle.load(open(\"embeddings_dict.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2123\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings_dict_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of embeddings that matches the size of the original dataset\n",
    "target_features = [embeddings_dict_merged[uniprot_id] for uniprot_id in df['target_uniprot_id']]\n",
    "\n",
    "data['target'].x = torch.tensor(target_features, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound node features shape torch.Size([46660, 1024])\n",
      "Target node features shape torch.Size([70902, 1024])\n",
      "Reverse edge index shape: torch.Size([2, 70902])\n",
      "Edge index shape: torch.Size([2, 70902])\n",
      "PPI edge index shape: torch.Size([2, 7140])\n"
     ]
    }
   ],
   "source": [
    "# Check the new feature shapes\n",
    "print(\"Compound node features shape\", data['compound'].x.shape)\n",
    "print(\"Target node features shape\", data['target'].x.shape)\n",
    "\n",
    "# Check shapes to verify\n",
    "print(\"Reverse edge index shape:\", data['compound', 'interacts', 'target'].edge_index.shape)\n",
    "print(\"Edge index shape:\", data['target', 'interacts', 'compound'].edge_index.shape)\n",
    "print(\"PPI edge index shape:\", data['target', 'interacts', 'target'].edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RandomLinkSplit to generate train, validation, and test sets\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.1,  # 10% validation edges\n",
    "    num_test=0.2,  # 20% test edges\n",
    "    is_undirected=False,  # Keep it directed, set to True for undirected\n",
    "    add_negative_train_samples=True,  # Generate negative samples for training\n",
    "    edge_types=('target', 'interacts', 'compound'),  # Specify the edge type\n",
    "    rev_edge_types=('compound', 'interacts', 'target')  # Specify the reverse edge type\n",
    ")\n",
    "\n",
    "# Perform train, val, and test split and move the splits to the GPU\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "# Move split datasets to the same device\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  compound={ x=[46660, 1024] },\n",
      "  target={ x=[70902, 1024] },\n",
      "  (target, interacts, compound)={\n",
      "    edge_index=[2, 49632],\n",
      "    edge_label=[99264],\n",
      "    edge_label_index=[2, 99264],\n",
      "  },\n",
      "  (compound, interacts, target)={ edge_index=[2, 49632] },\n",
      "  (target, interacts, target)={ edge_index=[2, 7140] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP for link prediction\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)  # Output is a single scalar score\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.sigmoid(self.fc2(x))  # Output between 0 and 1 (probability)\n",
    "\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=1):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.attn_weights = {}  # To store attention weights\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('target', 'interacts', 'compound'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('compound', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('target', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False)\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Batch normalization after first layer\n",
    "        self.batchnorm1_compound = BatchNorm1d(hidden_channels * heads)\n",
    "        self.batchnorm1_target = BatchNorm1d(hidden_channels * heads)\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('target', 'interacts', 'compound'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('compound', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('target', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False)\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Batch normalization after second layer\n",
    "        self.batchnorm2_compound = BatchNorm1d(hidden_channels * heads)\n",
    "        self.batchnorm2_target = BatchNorm1d(hidden_channels * heads)\n",
    "\n",
    "        # Define the MLP for link prediction\n",
    "        input_dim = hidden_channels * heads * 2  # Because we will concatenate embeddings of two nodes\n",
    "        self.link_predictor = LinkPredictor(input_dim, hidden_dim=hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, dir=True):\n",
    "        self.attn_weights = {}  # Clear attention weights at each forward pass\n",
    "        \n",
    "        # Forward pass through the first HeteroConv layer\n",
    "        x_dict, attn1 = self._apply_conv_and_extract_attention(self.conv1, x_dict, edge_index_dict)\n",
    "\n",
    "        x_dict['compound'] = F.relu(self.batchnorm1_compound(x_dict['compound']))\n",
    "        x_dict['target'] = F.relu(self.batchnorm1_target(x_dict['target']))\n",
    "\n",
    "        # Forward pass through the second HeteroConv layer\n",
    "        x_dict, attn2 = self._apply_conv_and_extract_attention(self.conv2, x_dict, edge_index_dict)\n",
    "\n",
    "        x_dict['compound'] = self.batchnorm2_compound(x_dict['compound'])\n",
    "        x_dict['target'] = self.batchnorm2_target(x_dict['target'])\n",
    "\n",
    "        if dir:\n",
    "            edge_index = edge_index_dict[('target', 'interacts', 'compound')]\n",
    "            source_x = x_dict['target'][edge_index[0]]  # Source node embeddings (compounds)\n",
    "            target_x = x_dict['compound'][edge_index[1]]    # Target node embeddings (targets)\n",
    "        else:\n",
    "            edge_index = edge_index_dict[('compound', 'interacts', 'target')]\n",
    "            source_x = x_dict['compound'][edge_index[0]]\n",
    "            target_x = x_dict['target'][edge_index[1]]\n",
    "\n",
    "        # Concatenate the embeddings of source and target nodes\n",
    "        x_concat = torch.cat([source_x, target_x], dim=-1).to(device)  # Concatenate along the feature dimension\n",
    "        \n",
    "        # Pass the concatenated embeddings through the neural network\n",
    "        return self.link_predictor(x_concat)  # Output a probability or score for the link\n",
    "\n",
    "    def _apply_conv_and_extract_attention(self, conv, x_dict, edge_index_dict):\n",
    "        # Initialize attention storage for this convolution layer\n",
    "        attn_weights = {}\n",
    "\n",
    "        # Iterate over each edge type in the heterogeneous graph\n",
    "        for edge_type, gat_conv in conv.convs.items():\n",
    "            edge_index = edge_index_dict[edge_type]\n",
    "\n",
    "            # Perform GATConv and retrieve attention weights\n",
    "            x_dict[edge_type[2]], attn = gat_conv((x_dict[edge_type[0]], x_dict[edge_type[2]]), edge_index, return_attention_weights=True)\n",
    "            attn_weights[edge_type] = attn[1]  # Store attention weights\n",
    "\n",
    "        # Save attention weights for this layer\n",
    "        self.attn_weights.update(attn_weights)\n",
    "        return x_dict, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GATModel(\n",
    "    hidden_channels=64,\n",
    "    heads=8\n",
    ")\n",
    "\n",
    "# Check for available device (GPU or CPU)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "def train(data, device):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Move the data to the GPU (if it's not already on the same device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Positive edges (compound -> target)\n",
    "    pos_link_pred = model(data.x_dict, data.edge_index_dict).squeeze()\n",
    "\n",
    "    # Initialize the neg_edge_index_dict\n",
    "    neg_edge_index_dict = {}\n",
    "\n",
    "    # Iterate through edge types in edge_index_dict\n",
    "    for edge_type, edge_index in data.edge_index_dict.items():\n",
    "        source_type, _, target_type = edge_type\n",
    "\n",
    "        # If edge type is 'target -> compound', generate negative samples\n",
    "        if edge_type == ('target', 'interacts', 'compound'):\n",
    "            neg_edge_index = negative_sampling(\n",
    "                edge_index=edge_index.to(device),\n",
    "                num_nodes=(data[source_type].num_nodes, data[target_type].num_nodes),\n",
    "                num_neg_samples=edge_index.size(1)  # 1:1 ratio of positive to negative samples\n",
    "            )\n",
    "            neg_edge_index_dict[edge_type] = neg_edge_index\n",
    "\n",
    "        # If edge type is 'target -> target', use the original edges (no sampling)\n",
    "        elif edge_type == ('target', 'interacts', 'target'):\n",
    "            neg_edge_index_dict[edge_type] = edge_index.to(device)\n",
    "        \n",
    "        else:\n",
    "            neg_edge_index_rev = negative_sampling(\n",
    "                edge_index=edge_index.to(device),\n",
    "                num_nodes=(data[source_type].num_nodes, data[target_type].num_nodes),\n",
    "                num_neg_samples=edge_index.size(1)  # 1:1 ratio of positive to negative samples\n",
    "            )\n",
    "            neg_edge_index_dict[edge_type] = neg_edge_index_rev\n",
    "\n",
    "    neg_link_pred = model(data.x_dict, neg_edge_index_dict).squeeze()\n",
    "\n",
    "    # Combine positive and negative samples for the loss\n",
    "    link_preds = torch.cat([pos_link_pred, neg_link_pred], dim=0)\n",
    "    link_labels = torch.cat([\n",
    "        torch.ones(pos_link_pred.size(0)).to(device),  # Positive samples on the same device\n",
    "        torch.zeros(neg_link_pred.size(0)).to(device)  # Negative samples on the same device\n",
    "    ], dim=0)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(link_preds, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, edge_label_index, edge_label, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Move data to GPU if available\n",
    "    edge_label_index = edge_label_index.to(device)\n",
    "    edge_label = edge_label.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Positive edges for test set\n",
    "        pos_link_logits = model(data.x_dict, data.edge_index_dict).squeeze()\n",
    "\n",
    "        neg_edge_index_dict = {}\n",
    "\n",
    "        # Iterate through edge types in edge_index_dict\n",
    "        for edge_type, edge_index in data.edge_index_dict.items():\n",
    "            source_type, _, target_type = edge_type\n",
    "\n",
    "            # If edge type is 'target -> compound', generate negative samples\n",
    "            if edge_type == ('target', 'interacts', 'compound'):\n",
    "                neg_edge_index = negative_sampling(\n",
    "                    edge_index=edge_index.to(device),\n",
    "                    num_nodes=(data[source_type].num_nodes, data[target_type].num_nodes),\n",
    "                    num_neg_samples=edge_index.size(1)  # 1:1 ratio of positive to negative samples\n",
    "                )\n",
    "                neg_edge_index_dict[edge_type] = neg_edge_index\n",
    "\n",
    "            # If edge type is 'target -> target', use the original edges (no sampling)\n",
    "            elif edge_type == ('target', 'interacts', 'target'):\n",
    "                neg_edge_index_dict[edge_type] = edge_index.to(device)\n",
    "            \n",
    "            else:\n",
    "                neg_edge_index_rev = negative_sampling(\n",
    "                    edge_index=edge_index.to(device),\n",
    "                    num_nodes=(data[source_type].num_nodes, data[target_type].num_nodes),\n",
    "                    num_neg_samples=edge_index.size(1)  # 1:1 ratio of positive to negative samples\n",
    "                )\n",
    "                neg_edge_index_dict[edge_type] = neg_edge_index_rev\n",
    "\n",
    "        neg_link_logits = model(data.x_dict, neg_edge_index_dict, dir=False).squeeze()\n",
    "\n",
    "        all_link_logits = torch.cat([pos_link_logits, neg_link_logits], dim=0)\n",
    "\n",
    "        all_link_labels = torch.cat([\n",
    "            torch.ones(pos_link_logits.size(0), device=device),\n",
    "            torch.zeros(neg_link_logits.size(0), device=device)\n",
    "        ], dim=0)\n",
    "\n",
    "        # Compute probabilities and predictions\n",
    "        link_probs = all_link_logits.cpu().numpy()\n",
    "        link_labels = all_link_labels.cpu().numpy()\n",
    "        \n",
    "        # Compute AUC\n",
    "        auc = roc_auc_score(link_labels, link_probs)\n",
    "        \n",
    "        # Convert probabilities to binary predictions\n",
    "        link_pred = (link_probs > 0.5).astype(int)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        acc = accuracy_score(link_labels, link_pred)\n",
    "    \n",
    "    return auc, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('target', 'interacts', 'compound'): tensor([[    0,     1,     1,  ...,   119,    53,    53],\n",
      "        [    0,     1,     2,  ..., 46657, 46658, 46659]]), ('compound', 'interacts', 'target'): tensor([[    0,     1,     2,  ..., 46657, 46658, 46659],\n",
      "        [    0,     1,     1,  ...,   119,    53,    53]]), ('target', 'interacts', 'target'): tensor([[ 912,  919,  912,  ...,  731,  731, 2122],\n",
      "        [ 919,  912,  920,  ..., 2122, 2122,  731]])}\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge type: ('target', 'interacts', 'compound'), Max Source Index: 1022, Max Target Index: 46659\n",
      "Edge type: ('compound', 'interacts', 'target'), Max Source Index: 46659, Max Target Index: 1022\n",
      "Edge type: ('target', 'interacts', 'target'), Max Source Index: 2122, Max Target Index: 2122\n"
     ]
    }
   ],
   "source": [
    "for edge_type, edge_index in data.edge_index_dict.items():\n",
    "    max_source = edge_index[0].max().item()\n",
    "    max_target = edge_index[1].max().item()\n",
    "    print(f\"Edge type: {edge_type}, Max Source Index: {max_source}, Max Target Index: {max_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Fold 1, Epoch 10, Loss: 0.2265, Val AUC: 0.1523, Val Acc: 0.2547\n",
      "Fold 1, Epoch 20, Loss: 0.0203, Val AUC: 0.1520, Val Acc: 0.2496\n",
      "Fold 1, Epoch 30, Loss: 0.1469, Val AUC: 0.0702, Val Acc: 0.2068\n",
      "Fold 1, Epoch 40, Loss: 0.0115, Val AUC: 0.0701, Val Acc: 0.2089\n",
      "Fold 1, Epoch 50, Loss: 0.0220, Val AUC: 0.0683, Val Acc: 0.2059\n",
      "Fold 1 Test AUC: 0.0722, Test Accuracy: 0.1812\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2, Epoch 10, Loss: 0.0092, Val AUC: 0.0665, Val Acc: 0.2054\n",
      "Fold 2, Epoch 20, Loss: 0.0051, Val AUC: 0.0664, Val Acc: 0.2078\n",
      "Fold 2, Epoch 30, Loss: 0.0185, Val AUC: 0.0548, Val Acc: 0.2002\n",
      "Fold 2, Epoch 40, Loss: 0.0000, Val AUC: 0.0214, Val Acc: 0.1873\n",
      "Fold 2, Epoch 50, Loss: 0.0031, Val AUC: 0.1058, Val Acc: 0.2304\n",
      "Fold 2 Test AUC: 0.1135, Test Accuracy: 0.2097\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3, Epoch 10, Loss: 0.0012, Val AUC: 0.1060, Val Acc: 0.2259\n",
      "Fold 3, Epoch 20, Loss: 0.0020, Val AUC: 0.1035, Val Acc: 0.2265\n",
      "Fold 3, Epoch 30, Loss: 0.0000, Val AUC: 0.1013, Val Acc: 0.2264\n",
      "Fold 3, Epoch 40, Loss: 0.0000, Val AUC: 0.1007, Val Acc: 0.2271\n",
      "Fold 3, Epoch 50, Loss: 0.2875, Val AUC: 0.1514, Val Acc: 0.2494\n",
      "Fold 3 Test AUC: 0.1593, Test Accuracy: 0.2294\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4, Epoch 10, Loss: 0.9559, Val AUC: 0.9898, Val Acc: 0.6733\n",
      "Fold 4, Epoch 20, Loss: 0.7422, Val AUC: 0.4526, Val Acc: 0.6735\n",
      "Fold 4, Epoch 30, Loss: 0.7409, Val AUC: 0.4467, Val Acc: 0.2503\n",
      "Fold 4, Epoch 40, Loss: 0.7445, Val AUC: 0.4456, Val Acc: 0.6729\n",
      "Fold 4, Epoch 50, Loss: 0.5504, Val AUC: 0.1545, Val Acc: 0.2501\n",
      "Fold 4 Test AUC: 0.1627, Test Accuracy: 0.2301\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5, Epoch 10, Loss: 0.5965, Val AUC: 0.4378, Val Acc: 0.2476\n",
      "Fold 5, Epoch 20, Loss: 0.7589, Val AUC: 0.4380, Val Acc: 0.6280\n",
      "Fold 5, Epoch 30, Loss: 0.6632, Val AUC: 0.4230, Val Acc: 0.2435\n",
      "Fold 5, Epoch 40, Loss: 0.6318, Val AUC: 0.1469, Val Acc: 0.2433\n",
      "Fold 5, Epoch 50, Loss: 0.7042, Val AUC: 0.4382, Val Acc: 0.2462\n",
      "Fold 5 Test AUC: 0.3990, Test Accuracy: 0.2238\n",
      "\n",
      "Average Test AUC: 0.1813, Average Test Accuracy: 0.2148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "\n",
    "# Define number of epochs and folds\n",
    "epochs = 50\n",
    "k_folds = 5\n",
    "\n",
    "# Extract edges for prediction\n",
    "edge_label_index = train_data['target', 'interacts', 'compound'].edge_label_index\n",
    "edge_labels = train_data['target', 'interacts', 'compound'].edge_label\n",
    "\n",
    "# Convert to NumPy for easier indexing\n",
    "edge_indices = edge_label_index.t().cpu().numpy()  # Shape [N, 2]\n",
    "edge_labels = edge_labels.cpu().numpy()  # Shape [N]\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(edge_indices), 1):\n",
    "    print(f'\\nFold {fold}/{k_folds}')\n",
    "\n",
    "    # Create train and validation subsets\n",
    "    train_edges = torch.tensor(edge_indices[train_idx]).T.to(device)\n",
    "    train_labels = torch.tensor(edge_labels[train_idx]).to(device)\n",
    "    \n",
    "    val_edges = torch.tensor(edge_indices[val_idx]).T.to(device)\n",
    "    val_labels = torch.tensor(edge_labels[val_idx]).to(device)\n",
    "\n",
    "    # Create a copy of train_data and update the edge_label sets\n",
    "    train_subset = train_data.clone()\n",
    "    train_subset['target', 'interacts', 'compound'].edge_label_index = train_edges\n",
    "    train_subset['target', 'interacts', 'compound'].edge_label = train_labels\n",
    "\n",
    "    val_subset = train_data.clone()\n",
    "    val_subset['target', 'interacts', 'compound'].edge_label_index = val_edges\n",
    "    val_subset['target', 'interacts', 'compound'].edge_label = val_labels\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train(train_subset, device)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            val_auc, val_acc = test(val_subset, val_subset['target', 'interacts', 'compound'].edge_label_index,\n",
    "                                    val_subset['target', 'interacts', 'compound'].edge_label, device)\n",
    "            print(f'Fold {fold}, Epoch {epoch}, Loss: {loss:.4f}, Val AUC: {val_auc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Evaluate on test data for this fold\n",
    "    test_auc, test_acc = test(test_data, test_data['target', 'interacts', 'compound'].edge_label_index,\n",
    "                              test_data['target', 'interacts', 'compound'].edge_label, device)\n",
    "    \n",
    "    print(f'Fold {fold} Test AUC: {test_auc:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "    fold_results.append((test_auc, test_acc))\n",
    "\n",
    "# Compute average results\n",
    "avg_auc = sum([res[0] for res in fold_results]) / k_folds\n",
    "avg_acc = sum([res[1] for res in fold_results]) / k_folds\n",
    "\n",
    "print(f'\\nAverage Test AUC: {avg_auc:.4f}, Average Test Accuracy: {avg_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target node count: 70902\n",
      "Compound node count: 46660\n"
     ]
    }
   ],
   "source": [
    "print(\"Target node count:\", data['target'].num_nodes)\n",
    "print(\"Compound node count:\", data['compound'].num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 proteins interacting with the protein-drug pair Q6GPI1 <-> CHEMBL4112929 :\n",
      "1. P15144\n",
      "2. P06307\n",
      "3. P01375\n",
      "4. P01011\n",
      "5. Q8TF68\n"
     ]
    }
   ],
   "source": [
    "# Retrieve attention weights for 'target' -> 'target' edges\n",
    "edge_type = ('target', 'interacts', 'target')\n",
    "attn_weights = model.attn_weights[edge_type]  # Shape: [num_edges, heads]\n",
    "mean_attn_weights = attn_weights.mean(dim=-1)  # Average across attention heads\n",
    "\n",
    "# Select a specific interaction to explain using CHEMBL IDs\n",
    "drug_chembl_id = 'CHEMBL4752635'  # Example CHEMBL ID for drug\n",
    "protein_uniprot_id = 'P14780'  # Example Protein ID for protein\n",
    "\n",
    "# Get the node indices for the drug and protein\n",
    "compound_index = compound_map[drug_chembl_id]\n",
    "protein_idx = target_map[protein_uniprot_id]\n",
    "\n",
    "# Get the edge index for the protein -> protein interaction\n",
    "edge_index = data['target', 'interacts', 'target'].edge_index\n",
    "\n",
    "# Get the attention weights and corresponding proteins for protein -> protein interactions involving the target protein\n",
    "attn_weights, protein_idx = mean_attn_weights[edge_index[1] == protein_idx], edge_index[0][edge_index[1] == protein_idx]\n",
    "\n",
    "# Get the top 5 proteins with the highest attention weights\n",
    "top_k = 5\n",
    "top_k_indices = protein_idx[attn_weights.argsort(descending=True)][:top_k]\n",
    "\n",
    "# Map the indices back to the protein uniprot IDs\n",
    "target_map_inv = {v: k for k, v in target_map.items()}\n",
    "top_k_proteins = [target_map_inv[i.item()] for i in top_k_indices]\n",
    "\n",
    "print(f\"Top 5 proteins interacting with the protein-drug pair {protein_uniprot_id} <-> {drug_chembl_id} :\")\n",
    "\n",
    "for i, protein in enumerate(top_k_proteins):\n",
    "    print(f\"{i + 1}. {protein}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2123\n"
     ]
    }
   ],
   "source": [
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Graph Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import VGAE\n",
    "from torch_geometric.utils import train_test_split_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder for VGAE\n",
    "class HeteroEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=1):\n",
    "        super(HeteroEncoder, self).__init__()\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('target', 'interacts', 'compound'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('compound', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('target', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False)\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.conv_mu = HeteroConv({\n",
    "            ('target', 'interacts', 'compound'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('compound', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('target', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False)\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.conv_logstd = HeteroConv({\n",
    "            ('target', 'interacts', 'compound'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('compound', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('target', 'interacts', 'target'): GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False)\n",
    "        }, aggr='mean')\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}  # Apply ReLU activation\n",
    "        \n",
    "        mu_dict = self.conv_mu(x_dict, edge_index_dict)  # Mean\n",
    "        logstd_dict = self.conv_logstd(x_dict, edge_index_dict)  # Log Standard Deviation \n",
    "\n",
    "        return mu_dict, logstd_dict\n",
    "\n",
    "class HeteroVGAE(VGAE):\n",
    "    def encode(self, x_dict, edge_index_dict):\n",
    "        mu_dict, logstd_dict = self.encoder(x_dict, edge_index_dict)\n",
    "\n",
    "        # Concatenate features of all node types into single tensors\n",
    "        mu = torch.cat([mu_dict[key] for key in mu_dict], dim=0)\n",
    "        logstd = torch.cat([logstd_dict[key] for key in logstd_dict], dim=0)\n",
    "\n",
    "        # Clamp logstd to prevent numerical instability\n",
    "        logstd = logstd.clamp(min=-10, max=10)\n",
    "\n",
    "        z = self.reparametrize(mu, logstd)\n",
    "\n",
    "        return z, mu, logstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGAE Model\n",
    "model = HeteroVGAE(HeteroEncoder(hidden_channels=64, heads=8)).to(device)\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 54.587955\n",
      "Epoch 20, Loss: 31.094702\n",
      "Epoch 40, Loss: 27.135923\n",
      "Epoch 60, Loss: 23.926014\n",
      "Epoch 80, Loss: 21.270927\n",
      "Epoch 100, Loss: 20.115210\n",
      "Epoch 120, Loss: 19.330423\n",
      "Epoch 140, Loss: 18.822206\n",
      "Epoch 160, Loss: 18.393713\n",
      "Epoch 180, Loss: 18.253078\n",
      "Epoch 200, Loss: 18.065180\n"
     ]
    }
   ],
   "source": [
    "# Training Function\n",
    "def train(train_data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z, mu, logstd = model.encode(train_data.x_dict, train_data.edge_index_dict)\n",
    "    \n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "\n",
    "    for edge_type, edge_index in train_data.edge_index_dict.items():\n",
    "        loss += model.recon_loss(z, edge_index) + (1 / (train_data[edge_type[0]].num_nodes + train_data[edge_type[2]].num_nodes)) * model.kl_loss(mu, logstd)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "epochs = 50\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(targets), 1):\n",
    "    print(f'\\nFold {fold}/{k_folds}')\n",
    "\n",
    "    # Create train and validation subsets\n",
    "    train_data_fold = train_data.clone()\n",
    "    val_data_fold = val_data.clone()\n",
    "\n",
    "    # Update edge_label_index and edge_label for train and val subsets\n",
    "    for edge_type, edge_index in train_data_fold.edge_index_dict.items():\n",
    "        train_data_fold[edge_type].edge_label_index = edge_index[:, train_idx]\n",
    "        train_data_fold[edge_type].edge_label = edge_index[:, train_idx]\n",
    "\n",
    "        val_data_fold[edge_type].edge_label_index = edge_index[:, val_idx]\n",
    "        val_data_fold[edge_type].edge_label = edge_index[:, val_idx]\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Fold {fold}, Epoch {epoch}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'hetero_vgae_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load('hetero_vgae_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2029,  0.3498,  0.2685,  ..., -0.2318, -0.2427, -0.3927],\n",
      "        [-0.2453,  0.5501,  0.5172,  ..., -0.5384, -0.5338, -1.0964],\n",
      "        [-0.2453,  0.5501,  0.5172,  ..., -0.5384, -0.5338, -1.0964],\n",
      "        ...,\n",
      "        [ 0.0220, -0.0260, -0.0344,  ...,  0.0141,  0.0285,  0.0295],\n",
      "        [ 0.0220, -0.0260, -0.0344,  ...,  0.0141,  0.0285,  0.0295],\n",
      "        [ 0.0220, -0.0260, -0.0344,  ...,  0.0141,  0.0285,  0.0295]],\n",
      "       device='cuda:0')\n",
      "56722\n",
      "[1. 1. 1. ... 1. 1. 1.] 56722\n",
      "Original Edges: 28360\n",
      "Newly Predicted Interactions: 56722\n"
     ]
    }
   ],
   "source": [
    "# Create all possible pairs (cartesian product)\n",
    "\n",
    "target_nodes = torch.arange(test_data['target'].num_nodes).to(device)\n",
    "compound_nodes = torch.arange(test_data['compound'].num_nodes).to(device)\n",
    "\n",
    "# possible_edges = torch.cartesian_prod(target_nodes, compound_nodes).t().contiguous().to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z, _, _ = model.encode(test_data.x_dict, test_data.edge_index_dict)\n",
    "    print(z)\n",
    "    new_edges = model.decode(z, test_data.edge_index_dict['target', 'interacts', 'compound'])\n",
    "\n",
    "# Convert Predicted Edges to a List\n",
    "print(len(test_data.edge_index_dict[('target', 'interacts', 'compound')][0]))\n",
    "predicted_edges = new_edges.cpu().numpy()\n",
    "print(predicted_edges, len(predicted_edges))\n",
    "threshold = 0.5\n",
    "new_interactions = (predicted_edges > threshold).astype(int)\n",
    "\n",
    "# Compare Old and New Graph\n",
    "print(f\"Original Edges: {len(test_data['target', 'interacts', 'compound'].edge_label)}\")\n",
    "print(f\"Newly Predicted Interactions: {sum(new_interactions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
